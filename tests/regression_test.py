import pytest
import logging
from datetime import datetime
import os
import json
from unittest.mock import Mock, patch
from sukoon import (
    route_query, 
    run_conversational_agent, 
    run_suicide_prevention_agent, 
    chat,
    State
)
from langchain_core.messages import HumanMessage, AIMessage

# Set up logging for regression tests
def setup_regression_logging():
    log_dir = 'regression_logs'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = f'{log_dir}/regression_test_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_regression_logging()

# Baseline responses for regression testing
BASELINE_RESPONSES = {
    'greeting': "Hello! How can I help you today?",
    'crisis': "I understand you're going through a difficult time. I'm here to support you.",
    'general': "I appreciate you sharing that with me.",
}

# Test cases for regression testing
REGRESSION_TEST_CASES = [
    {
        'id': 'basic_greeting',
        'input': "Hello",
        'expected_route': 'conversational',
        'expected_response_type': 'greeting'
    },
    {
        'id': 'crisis_detection',
        'input': "I'm thinking about ending it all",
        'expected_route': 'suicide_prevention',
        'expected_response_type': 'crisis'
    },
    {
        'id': 'general_conversation',
        'input': "I had a good day today",
        'expected_route': 'conversational',
        'expected_response_type': 'general'
    }
]

class RegressionTestResult:
    def __init__(self):
        self.passed = True
        self.failures = []
        self.test_count = 0
        self.timestamp = datetime.now()

    def add_failure(self, test_id, details):
        self.passed = False
        self.failures.append({'test_id': test_id, 'details': details})

    def increment_count(self):
        self.test_count += 1

    def to_dict(self):
        return {
            'timestamp': self.timestamp.isoformat(),
            'passed': self.passed,
            'total_tests': self.test_count,
            'failures': self.failures
        }

@pytest.fixture
def regression_result():
    return RegressionTestResult()

@pytest.mark.regression
class TestSukoonRegression:
    """Regression test suite for Sukoon application"""

    def save_regression_results(self, result: RegressionTestResult):
        """Save regression test results to a JSON file"""
        results_dir = 'regression_results'
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'{results_dir}/regression_results_{timestamp}.json'
        
        with open(filename, 'w') as f:
            json.dump(result.to_dict(), f, indent=2)
        
        logger.info(f"Regression test results saved to {filename}")

    @pytest.mark.parametrize('test_case', REGRESSION_TEST_CASES)
    def test_core_functionality(self, mock_openai, regression_result, test_case):
        """Test core functionality hasn't regressed"""
        logger.info(f"Running regression test: {test_case['id']}")
        regression_result.increment_count()

        try:
            # Test routing
            state = State(messages=[HumanMessage(content=test_case['input'])])
            mock_openai.return_value.with_structured_output.return_value.invoke.return_value.route = test_case['expected_route']
            route = route_query(state)
            
            assert route == test_case['expected_route'], f"Routing failed for {test_case['id']}"
            
            # Test response generation
            expected_response = BASELINE_RESPONSES[test_case['expected_response_type']]
            mock_openai.return_value.invoke.return_value = expected_response
            
            if route == 'conversational':
                response = run_conversational_agent(state)
            else:
                response = run_suicide_prevention_agent(state)
            
            assert isinstance(response, dict), f"Response format invalid for {test_case['id']}"
            assert "messages" in response, f"Response missing messages key for {test_case['id']}"
            
            logger.info(f"Test {test_case['id']} passed")
            
        except AssertionError as e:
            regression_result.add_failure(test_case['id'], str(e))
            logger.error(f"Regression test failed: {test_case['id']} - {str(e)}")
            raise

    def test_error_handling_regression(self, mock_openai, regression_result):
        """Test error handling hasn't regressed"""
        logger.info("Running error handling regression test")
        regression_result.increment_count()
        
        try:
            # Test API error handling
            state = State(messages=[HumanMessage(content="Test message")])
            mock_openai.return_value.invoke.side_effect = Exception("API Error")
            
            with pytest.raises(Exception) as exc_info:
                run_conversational_agent(state)
            
            assert str(exc_info.value) == "API Error"
            logger.info("Error handling test passed")
            
        except AssertionError as e:
            regression_result.add_failure('error_handling', str(e))
            logger.error(f"Error handling regression test failed: {str(e)}")
            raise

    def test_message_format_regression(self, mock_openai, regression_result):
        """Test message format hasn't regressed"""
        logger.info("Running message format regression test")
        regression_result.increment_count()
        
        try:
            state = State(messages=[HumanMessage(content="Test message")])
            
            # Test conversational message format
            mock_openai.return_value.invoke.return_value = BASELINE_RESPONSES['greeting']
            response = run_conversational_agent(state)
            
            assert isinstance(response, dict), "Response not a dictionary"
            assert "messages" in response, "Response missing messages key"
            assert isinstance(response["messages"], str), "Message not a string"
            
            logger.info("Message format test passed")
            
        except AssertionError as e:
            regression_result.add_failure('message_format', str(e))
            logger.error(f"Message format regression test failed: {str(e)}")
            raise

    @pytest.fixture(autouse=True)
    def run_around_tests(self, regression_result):
        """Save regression results after all tests complete"""
        yield
        self.save_regression_results(regression_result)

if __name__ == "__main__":
    pytest.main([
        __file__,
        "-v",
        "--html=regression_reports/report.html",
        "-m", "regression"
    ])